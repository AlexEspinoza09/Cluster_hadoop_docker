# Estimaci√≥n de Pi - Prueba de Procesamiento CPU Intensivo

## Prueba Completada Exitosamente

Acabas de ejecutar un trabajo de **estimaci√≥n de Pi usando el m√©todo Monte Carlo** que distribuy√≥ 20 tareas de procesamiento intensivo de CPU entre tus 3 nodos.

## M√©tricas de Rendimiento

### Tiempo de Ejecuci√≥n
- **Fase Map**: ~146 segundos (2.4 minutos)
- **Fase Reduce**: ~2 segundos
- **Total**: 149.76 segundos (~2.5 minutos)

### Resultado de la Estimaci√≥n
- **Valor estimado de Pi**: 3.14118
- **Valor real de Pi**: 3.14159265...
- **Precisi√≥n**: 99.97% de exactitud
- **Error**: 0.00041 (muy preciso!)

### Procesamiento Distribuido
- **Tareas Map lanzadas**: 20
- **Tareas Reduce lanzadas**: 1
- **Muestras por Map**: 10,000
- **Muestras totales**: 200,000 puntos aleatorios

### Distribuci√≥n de Trabajo
- **Data-local map tasks**: 7 (ejecutadas en el mismo nodo donde est√°n los datos)
- **Rack-local map tasks**: 13 (ejecutadas en nodos cercanos)
- **Total tiempo CPU**: 100.02 segundos (1 minuto 40 segundos)
- **Total tiempo Map**: 1,030,070 ms (17 minutos sumando todas las tareas)

### Uso de Recursos
- **Memoria f√≠sica pico total**: 6.68 GB
- **Memoria pico por Map**: ~337 MB
- **Memoria pico por Reduce**: ~229 MB
- **Tiempo de GC (Garbage Collection)**: 33.18 segundos
- **Virtual cores-milliseconds**: 1,071,028 (Map + Reduce combinados)

## C√≥mo Funciona el M√©todo Monte Carlo

### Concepto
El m√©todo Monte Carlo para estimar œÄ funciona generando puntos aleatorios en un cuadrado y contando cu√°ntos caen dentro de un c√≠rculo inscrito:

```
Cuadrado de lado 1
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ    ‚ï≠‚îÄ‚îÄ‚îÄ‚ïÆ    ‚îÇ
    ‚îÇ  ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ ‚îÇ  Radio = 1
    ‚îÇ ‚îÇ    ‚Ä¢‚Ä¢‚Ä¢    ‚îÇ ‚îÇ
    ‚îÇ ‚îÇ   ‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢   ‚îÇ ‚îÇ  C√≠rculo inscrito
    ‚îÇ ‚îÇ   ‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢   ‚îÇ ‚îÇ  √Årea = œÄ √ó r¬≤
    ‚îÇ  ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Cuadrado: √Årea = 4

Relaci√≥n: Puntos en c√≠rculo / Puntos totales ‚âà œÄ/4
Por lo tanto: œÄ ‚âà 4 √ó (puntos en c√≠rculo / puntos totales)
```

### Distribuci√≥n del Trabajo
```
Master divide en 20 tareas
         ‚Üì
    Cada tarea genera 10,000 puntos aleatorios
         ‚Üì
    Cuenta cu√°ntos caen dentro del c√≠rculo
         ‚Üì
    Tareas distribuidas entre 3 nodos (7 local + 13 rack-local)
         ‚Üì
    Reducer agrega todos los resultados
         ‚Üì
    Calcula estimaci√≥n final de Pi
```

## Comparaci√≥n con WordCount

| M√©trica | WordCount | Pi Estimation |
|---------|-----------|---------------|
| **Tipo** | I/O intensivo | CPU intensivo |
| **Tiempo total** | 28 segundos | 150 segundos |
| **Tiempo Map** | 19 segundos | 146 segundos |
| **Tiempo CPU** | 13 segundos | 100 segundos |
| **Datos procesados** | 6.2 MB | 2.3 KB (entrada peque√±a) |
| **Memoria pico** | 551 MB | 6.68 GB |
| **Tareas Map** | Variable | 20 tareas |

**Observaci√≥n clave**:
- WordCount es m√°s r√°pido porque procesa datos (I/O)
- Pi Estimation es m√°s lento porque hace c√°lculos intensivos (CPU)
- Pi Estimation usa m√°s memoria porque mantiene arrays de puntos aleatorios

## Distribuci√≥n Entre Nodos

### Data-local (7 tareas)
Estas tareas se ejecutaron en el mismo nodo donde estaban los datos de entrada, minimizando transferencia de red.

### Rack-local (13 tareas)
Estas tareas se ejecutaron en nodos diferentes, pero YARN optimiz√≥ la ubicaci√≥n para minimizar latencia.

**Esto demuestra**:
1. YARN est√° balanceando la carga entre los 3 nodos
2. El cluster est√° procesando tareas en paralelo
3. La localidad de datos est√° siendo optimizada autom√°ticamente

## Ver Resultados en las Interfaces Web

### ResourceManager - Ver el Job
**URL**: http://localhost:8088/cluster/apps

Busca la aplicaci√≥n "QuasiMonteCarlo" (application_1768261826278_0002)

### Job History - Detalles del Job
**URL**: http://localhost:19888/jobhistory/job/job_1768261826278_0002

Aqu√≠ puedes ver:
- Detalles de cada una de las 20 tareas Map
- Timeline de ejecuci√≥n
- Gr√°ficos de uso de CPU y memoria
- Distribuci√≥n de tareas entre nodos

## Lo Que Demuestra Esta Prueba

1. **Procesamiento CPU distribuido**: 20 tareas ejecut√°ndose en paralelo
2. **Balanceo de carga**: YARN distribuy√≥ las tareas entre los 3 nodos
3. **Escalabilidad de c√≥mputo**: M√°s nodos = m√°s tareas en paralelo = resultados m√°s r√°pidos
4. **Optimizaci√≥n de localidad**: 7 tareas data-local muestran optimizaci√≥n de YARN
5. **Gesti√≥n de recursos**: Picos de memoria controlados autom√°ticamente

## Por Qu√© Este Test es Importante

### 1. Tipo de Carga Diferente
- WordCount testa I/O (lectura/escritura)
- Pi Estimation testa CPU (c√°lculos)
- Juntos demuestran que el cluster maneja ambos tipos de carga

### 2. Paralelismo Real
20 tareas Map significa que el cluster puede ejecutar hasta 20 c√°lculos simult√°neos (limitado por recursos disponibles)

### 3. Precisi√≥n Matem√°tica
El resultado (œÄ ‚âà 3.14118) con solo 200,000 muestras demuestra que los c√°lculos son correctos y reproducibles.

## Comandos √ötiles

### Ver Detalles del Job
```bash
docker exec -u hadoop hadoop-master yarn application -status application_1768261826278_0002
```

### Ver Logs de una Tarea Espec√≠fica
```bash
docker exec -u hadoop hadoop-master yarn logs -applicationId application_1768261826278_0002
```

### Ver Uso de Recursos Durante Ejecuci√≥n
```bash
docker exec -u hadoop hadoop-master yarn top
```

## Pr√≥ximos Benchmarks Sugeridos

### 1. Aumentar Precisi√≥n de Pi
```bash
docker exec -u hadoop hadoop-master bash -c "hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar pi 50 100000"
```
- 50 Maps (m√°s paralelismo)
- 100,000 muestras por map
- Total: 5,000,000 puntos
- Resultado m√°s preciso, m√°s tiempo de ejecuci√≥n

### 2. TestDFSIO - Benchmark de I/O
```bash
# Escritura
docker exec -u hadoop hadoop-master bash -c "hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 100MB"

# Lectura
docker exec -u hadoop hadoop-master bash -c "hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 100MB"
```
Mide MB/s de throughput de HDFS

### 3. TeraSort - Benchmark de Ordenamiento
```bash
# Generar 10 GB de datos aleatorios
docker exec -u hadoop hadoop-master bash -c "hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar teragen 100000000 /terasort-input"

# Ordenar
docker exec -u hadoop hadoop-master bash -c "hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar terasort /terasort-input /terasort-output"
```
Benchmark est√°ndar de la industria

## An√°lisis de Rendimiento

### ¬øPor Qu√© Tom√≥ 150 Segundos?

1. **Startup overhead**: ~25 segundos para iniciar contenedores de tareas
2. **C√°lculos CPU**: 100 segundos de tiempo real de CPU
3. **GC (Garbage Collection)**: 33 segundos limpiando memoria
4. **Shuffle y Reduce**: 2 segundos agregando resultados

### ¬øC√≥mo Mejorar el Rendimiento?

1. **A√±adir m√°s nodos**: Con 6 nodos, las 20 tareas terminar√≠an ~50% m√°s r√°pido
2. **M√°s vCores por nodo**: Permite m√°s tareas concurrentes por nodo
3. **M√°s memoria**: Reduce tiempo de GC
4. **Optimizar par√°metros JVM**: Ajustar heap size y GC algorithm

## Aprendizajes Clave

1. **Monte Carlo es CPU-bound**: El tiempo de ejecuci√≥n est√° dominado por c√°lculos, no I/O
2. **Paralelismo funciona**: 20 tareas en 3 nodos procesando simult√°neamente
3. **Precisi√≥n vs Tiempo**: M√°s muestras = m√°s precisi√≥n = m√°s tiempo
4. **YARN gestiona recursos**: Distribuci√≥n autom√°tica de 20 tareas entre nodos disponibles
5. **Garbage Collection importa**: 33s de GC en 150s total = 22% del tiempo

---

**Tu cluster proces√≥ 200,000 puntos aleatorios distribuidos entre 3 nodos y estim√≥ Pi con 99.97% de precisi√≥n!** üéØ

**Application ID**: application_1768261826278_0002
**Job ID**: job_1768261826278_0002
**Estado**: FINISHED / SUCCEEDED

Visita http://localhost:8088 o http://localhost:19888 para ver m√°s detalles.
